# Machine Learning Documentation

Complete guide to the ML meta-labeling system used for strategy signal filtering.

---

## Overview

The Rooney Capital ML system uses **ensemble meta-labeling** (LightGBM + Random Forest) to filter trading signals generated by the base IBS strategy. Instead of directly predicting market direction, the ML models predict whether a given signal is likely to be profitable based on market conditions captured through 30+ engineered features.

**Key Characteristics:**
- **Meta-labeling approach**: ML predicts trade quality, not market direction
- **Three-way data split**: Prevents overfitting with separate train/validation/test sets
- **CPCV validation**: Combinatorial Purged Cross-Validation for time-series data
- **Deflated Sharpe Ratio**: Statistical correction for multiple testing bias
- **Time-based embargo**: Prevents label leakage from overlapping positions
- **Portfolio integration**: Models operate independently, optimized at portfolio level

---

## Complete ML Workflow

```
1. Strategy Research â†’ 2. Data Extraction â†’ 3. Model Training â†’ 4. Portfolio Optimization â†’ 5. Deployment
```

### 1. Strategy Research (Strategy Factory)
Discover profitable base strategies using systematic parameter optimization.
- See: [STRATEGY_FACTORY_GUIDE.md](../../STRATEGY_FACTORY_GUIDE.md)
- Output: Optimal strategy parameters per instrument

### 2. Data Extraction
Extract training features from historical backtests with optimal base strategy parameters.
- See: [DATA_TRANSFORMATION_TO_ML_WORKFLOW.md](DATA_TRANSFORMATION_TO_ML_WORKFLOW.md)
- Tool: `research/extract_training_data.py`
- Output: `data/training/{SYMBOL}_training_data.csv` with 30+ features

### 3. Model Training
Train meta-labeling models using three-way temporal split with rigorous validation.
- See: [THREE_WAY_SPLIT_GUIDE.md](THREE_WAY_SPLIT_GUIDE.md) (methodology)
- See: [END_TO_END_OPTIMIZATION_GUIDE.md](END_TO_END_OPTIMIZATION_GUIDE.md) (complete workflow)
- Tool: `research/ml_meta_labeling/train_rf_three_way_split.py`
- Output: `src/models/{SYMBOL}_rf_model.pkl` and `{SYMBOL}_best.json`

### 4. Portfolio Optimization
Select optimal strategy subset and position sizing using greedy optimization.
- See: [../portfolio/PORTFOLIO_INTEGRATION_GUIDE.md](../portfolio/PORTFOLIO_INTEGRATION_GUIDE.md)
- Tool: `research/portfolio_optimization/greedy_optimizer.py`
- Output: Portfolio configuration for `config.yml`

### 5. Deployment
Deploy trained models to production with live strategy execution.
- See: [../../LIVE_LAUNCH_GUIDE.md](../../LIVE_LAUNCH_GUIDE.md)
- See: [../../SYSTEM_GUIDE.md](../../SYSTEM_GUIDE.md)

---

## Documentation Index

### ðŸš€ Quick Start
**[STRATEGY_OPTIMIZATION_QUICKSTART.md](STRATEGY_OPTIMIZATION_QUICKSTART.md)**
Fast-track guide for experienced users. Covers the essential commands to go from raw data to trained models in under 30 minutes.

**When to use**: You understand the methodology and just need the command sequences.

---

### ðŸ“– Complete Workflow Guide
**[END_TO_END_OPTIMIZATION_GUIDE.md](END_TO_END_OPTIMIZATION_GUIDE.md)**
Comprehensive end-to-end guide covering:
- Data extraction from backtests
- Feature engineering and selection
- Model training with three-way split
- Hyperparameter optimization (Random Search â†’ Bayesian Optimization)
- Threshold optimization on validation set
- Final evaluation on hold-out test set
- Deployment to production

**When to use**: First-time training, troubleshooting, or understanding the complete pipeline.

---

### ðŸ”¬ Methodology Deep Dives

#### **[THREE_WAY_SPLIT_GUIDE.md](THREE_WAY_SPLIT_GUIDE.md)**
**Topic**: Training/Validation/Test data splitting methodology

**Covers**:
- Why three-way temporal split prevents overfitting
- How to split data chronologically (e.g., 2010-2018 / 2019-2020 / 2021-2024)
- Hyperparameter tuning on training set with CPCV
- Threshold optimization on validation set
- Final evaluation on unseen test set
- Time-based embargo windows (vs percentage-based purging)

**When to use**: Understanding the statistical rigor behind the training process or debugging overfitting issues.

---

#### **[CLUSTERED_FEATURE_SELECTION_GUIDE.md](CLUSTERED_FEATURE_SELECTION_GUIDE.md)**
**Topic**: Feature engineering and selection using correlation clustering

**Covers**:
- 30+ engineered features (IBS derivatives, cross-symbol correlations, volatility measures)
- Spearman correlation clustering to reduce multicollinearity
- Selecting representative features from each cluster
- Feature importance analysis with Random Forest
- Handling missing cross-symbol data gracefully

**When to use**: Adding new features, debugging feature calculation issues, or understanding what the models "see."

---

#### **[BASE_STRATEGY_OPTIMIZATION_README.md](BASE_STRATEGY_OPTIMIZATION_README.md)**
**Topic**: Base strategy parameter tuning (runs BEFORE ML training)

**Covers**:
- Optimizing IBS thresholds, position sizing, and exit rules
- Grid search vs. Bayesian optimization trade-offs
- Walk-forward analysis for robustness testing
- Selecting parameters that maximize Sharpe ratio
- Why base strategy must be optimized separately from ML layer

**When to use**: Researching new base strategies or retuning existing strategy parameters.

---

#### **[DATA_TRANSFORMATION_TO_ML_WORKFLOW.md](DATA_TRANSFORMATION_TO_ML_WORKFLOW.md)**
**Topic**: Data pipeline architecture from backtest to ML-ready features

**Covers**:
- How `extract_training_data.py` runs backtests with instrumentation
- Feature calculation during backtest execution
- Triple-barrier labeling for target generation
- Data quality checks and validation
- Output format specifications

**When to use**: Debugging data extraction issues, adding new features to the pipeline, or understanding feature lineage.

---

## Key Concepts

### Meta-Labeling
Traditional ML predicts market direction (up/down). Meta-labeling predicts **whether a discretionary signal will be profitable**. This is more robust because:
- Base strategy provides domain expertise (IBS mean-reversion edge)
- ML learns market conditions where the edge is strongest
- Separates signal generation from signal filtering

### Three-Way Temporal Split
```
[â•â•â•â•â•â•â•â• Training (2010-2018) â•â•â•â•â•â•â•â•] [â•â•â• Validation (2019-2020) â•â•â•] [â•â•â• Test (2021-2024) â•â•â•]
         â†“                                          â†“                              â†“
   Hyperparameter Tuning                  Threshold Optimization          Final Performance Eval
   (CPCV with embargo)                    (Single pass)                   (Never touched until end)
```

**Why this matters**:
- Training set: Tune model complexity (n_estimators, max_depth, min_samples_split)
- Validation set: Tune decision threshold (probability cutoff for taking trades)
- Test set: Unbiased estimate of true out-of-sample performance

### Combinatorial Purged Cross-Validation (CPCV)
Standard k-fold CV leaks information in time-series data because adjacent samples are correlated. CPCV:
- Splits data chronologically into k folds
- For each test fold, trains on all other folds **except those within embargo window**
- Embargo window = max holding period + buffer (typically 3 days for this system)
- Prevents using "future" information to predict "past" trades

### Deflated Sharpe Ratio (DSR)
When testing 420 model configurations (120 random + 300 Bayesian trials), the best result is biased upward due to selection from multiple attempts. DSR corrects for this:
- Adjusts Sharpe ratio for number of trials (`m` parameter)
- Accounts for return distribution skewness and kurtosis
- Provides statistically honest performance estimate
- Formula: `DSR = (SR - E[max SR under null]) / SE[SR]`

---

## Common Workflows

### Train a Single Model from Scratch
```bash
# 1. Extract training data
python research/extract_training_data.py --symbol ES --start 2010-01-01 --end 2024-12-31

# 2. Train model with three-way split
python research/ml_meta_labeling/train_rf_three_way_split.py \
    --symbol ES \
    --train-end 2018-12-31 \
    --threshold-end 2020-12-31 \
    --rs-trials 120 \
    --bo-trials 300

# 3. Model saved to src/models/ES_rf_model.pkl and ES_best.json
```

### Retrain All Models (Nightly Workflow)
```bash
# Run for all active instruments
for symbol in ES NQ RTY YM CL HG SI; do
    python research/extract_training_data.py --symbol $symbol --start 2010-01-01 --end 2024-12-31
    python research/ml_meta_labeling/train_rf_three_way_split.py \
        --symbol $symbol \
        --train-end 2018-12-31 \
        --threshold-end 2020-12-31 \
        --rs-trials 120 \
        --bo-trials 300
done
```

### Evaluate Model Performance (Post-Training)
```bash
# Check test set metrics (2021-2024 hold-out)
cat src/models/ES_best.json | grep -A 10 "test_metrics"

# Expected output:
# - Sharpe ratio (test set)
# - Win rate
# - Average profit per trade
# - Max drawdown
# - Deflated Sharpe Ratio
```

### Debug Missing Features
```bash
# Run feature verification script
python research/validation/verify_features.py

# Check which features are missing
# Common issues:
# - Cross-symbol feeds not loaded (check contract map)
# - VIX data unavailable (remove from feature list)
# - Derived features (rsixatrz, rsixvolz) missing base components
```

---

## File Locations

### Input Data
- **Historical data**: `data/databento/` (not in Git, local only)
- **Contract specifications**: `Data/Databento_contract_map.yml`

### Training Outputs
- **Extracted features**: `data/training/{SYMBOL}_training_data.csv`
- **Trained models**: `src/models/{SYMBOL}_rf_model.pkl` (Git LFS)
- **Model metadata**: `src/models/{SYMBOL}_best.json` (Git)
- **Training logs**: `logs/ml_training/`

### Production Models
- **Server location**: `/opt/pine/rooney-capital-v1/src/models/`
- **Loaded by**: `src/models/loader.py` in live worker
- **Version control**: Git LFS for *.pkl files

---

## Transaction Costs

All ML training uses realistic transaction costs matching live execution:

- **Commission**: $1.00 per side ($2.00 round trip)
- **Slippage**: 1 tick per order (2 ticks round trip)
  - ES: 0.25 points = $12.50 per contract
  - NQ: 0.25 points = $5.00 per contract
  - RTY: 0.10 points = $5.00 per contract
  - CL: 0.01 points = $10.00 per contract

**Total ES round trip cost**: $2.00 (commission) + $25.00 (slippage) = **$27.00**

See: [../operations/TRANSACTION_COST_CONFIGURATION.md](../operations/TRANSACTION_COST_CONFIGURATION.md)

---

## Quality Assurance

### Look-Ahead Bias Audit
Comprehensive review of potential data leakage:
- [../quality_assurance/LOOK_AHEAD_BIAS_REVIEW.md](../quality_assurance/LOOK_AHEAD_BIAS_REVIEW.md)

**Result**: âœ… No look-ahead bias detected in production pipeline

### Overfitting Prevention Checklist
- âœ… Three-way temporal split (train/validation/test)
- âœ… CPCV with 3-day embargo window
- âœ… Deflated Sharpe Ratio for multiple testing correction
- âœ… Threshold optimization on separate validation set
- âœ… Hold-out test set never touched until final report
- âœ… Walk-forward analysis for base strategy parameters
- âœ… Feature selection using domain knowledge + correlation clustering

---

## Troubleshooting

### Model Training Fails
**Symptom**: `train_rf_three_way_split.py` crashes or produces poor results

**Common causes**:
1. **Insufficient data**: Need minimum 3 years for three-way split
   - Solution: Extend date range or use two-way split for newer instruments
2. **Missing features**: Cross-symbol feeds unavailable during extraction
   - Solution: Run `verify_features.py`, check contract map
3. **Imbalanced classes**: Too few positive labels (profitable trades)
   - Solution: Adjust base strategy parameters to increase trade frequency
4. **Memory errors**: Large datasets with 300 BO trials
   - Solution: Reduce `--bo-trials` or use smaller `--n-splits` for CPCV

### Poor Test Set Performance
**Symptom**: Validation Sharpe = 1.8, Test Sharpe = 0.4

**Common causes**:
1. **Overfitting to validation set**: Threshold was over-optimized
   - Solution: Use coarser threshold grid (0.05 steps instead of 0.01)
2. **Market regime change**: 2021-2024 different from 2019-2020
   - Solution: Use more recent validation period or shorter training window
3. **Feature drift**: Cross-symbol correlations changed
   - Solution: Retrain more frequently or use simpler feature set

### Features Not Calculating in Live Trading
**Symptom**: `verify_features.py` shows 100% but live worker shows missing features

**Common causes**:
1. **Feed timing**: Cross-symbol feeds not loaded when strategy initializes
   - Solution: Check `METAL_ENERGY_SYMBOLS` includes required symbols
2. **Data feed failures**: Databento connection issues during warmup
   - Solution: Check logs for "Missing data feed" warnings
3. **Model version mismatch**: Deployed model expects features not in current code
   - Solution: Ensure `git pull` on server, verify model file dates match

---

## Performance Expectations

### Typical Model Metrics (Per Instrument)

**Training Set (2010-2018)**:
- Sharpe Ratio: 1.8 - 2.4
- Win Rate: 58% - 65%
- Avg Profit/Trade: $50 - $120 (varies by instrument)

**Validation Set (2019-2020)**:
- Sharpe Ratio: 1.5 - 2.0 (10-20% decay from training)
- Win Rate: 55% - 62%
- Threshold typically: 0.50 - 0.65

**Test Set (2021-2024)**:
- Sharpe Ratio: 1.2 - 1.8 (conservative estimate for live)
- Win Rate: 53% - 60%
- Deflated Sharpe: 1.0 - 1.5 (after multiple testing correction)

**Portfolio Level**:
- Target Sharpe: > 1.5 (ensemble of 7-9 instruments)
- Max Drawdown: < 15%
- Correlation between instruments: 0.3 - 0.7

---

## Next Steps

1. **First time training?** â†’ Start with [STRATEGY_OPTIMIZATION_QUICKSTART.md](STRATEGY_OPTIMIZATION_QUICKSTART.md)
2. **Need full context?** â†’ Read [END_TO_END_OPTIMIZATION_GUIDE.md](END_TO_END_OPTIMIZATION_GUIDE.md)
3. **Ready to deploy?** â†’ See [../../LIVE_LAUNCH_GUIDE.md](../../LIVE_LAUNCH_GUIDE.md)
4. **Building portfolio?** â†’ See [../portfolio/PORTFOLIO_INTEGRATION_GUIDE.md](../portfolio/PORTFOLIO_INTEGRATION_GUIDE.md)

---

## References

- Bailey, D. H., & LÃ³pez de Prado, M. (2014). "The Deflated Sharpe Ratio: Correcting for Selection Bias, Backtest Overfitting, and Non-Normality"
- LÃ³pez de Prado, M. (2018). "Advances in Financial Machine Learning" - Chapter 7: Cross-Validation in Finance
- Henrique et al. (2019). "Literature Review: Machine Learning Techniques Applied to Financial Market Prediction"
