# Redis Pub/Sub vs Queue Implementation Comparison

## Overview

This document compares the legacy **QueueFanout** (in-memory queue) approach with the new **Redis pub/sub** approach for distributing market data from the data hub to strategy workers.

---

## Architecture Comparison

### Legacy Queue Approach

```
DatabentoSubscriber
‚îú‚îÄ‚îÄ Aggregates trades ‚Üí 1-minute bars
‚îú‚îÄ‚îÄ QueueFanout.publish_bar(bar)
‚îî‚îÄ‚îÄ Per-symbol queues (maxsize=2048)
    ‚îî‚îÄ‚îÄ DatabentoLiveData reads from queue
```

**Key Features:**
- In-memory Python `queue.Queue` per symbol
- Thread-safe (uses locks)
- Bounded size (default 2048 bars)
- Blocking `queue.get(timeout)` for consumers
- QueueSignal for control messages (RESET, SHUTDOWN)

### New Redis Approach

```
DataHub
‚îú‚îÄ‚îÄ Aggregates trades ‚Üí 1-min/hourly/daily bars
‚îú‚îÄ‚îÄ RedisClient.publish_bar(symbol, timeframe, bar_data)
‚îî‚îÄ‚îÄ Redis pub/sub channels (market:{symbol}:{timeframe})
    ‚îî‚îÄ‚îÄ RedisLiveData subscribes to channels
```

**Key Features:**
- Network-based Redis pub/sub
- Unlimited queue size (Redis handles buffering)
- Non-blocking publish
- Subscriber thread with `pubsub.listen()` loop
- No built-in control signals

---

## Detailed Feature Comparison

### 1. Queue Size & Overflow Handling

**Legacy (QueueFanout):**
```python
def __init__(self, product_to_root: Mapping[str, str], maxsize: int = 2048):
    self._queues: Dict[str, "queue.Queue[Bar | QueueSignal]"] = {}
    self._maxsize = maxsize

def publish_bar(self, bar: Bar) -> None:
    q = self._ensure_queue(bar.symbol)
    try:
        q.put_nowait(bar)
    except queue.Full:
        logger.warning(
            "Queue for symbol %s is full (%d/%d); dropping bar at %s",
            bar.symbol,
            q.qsize(),
            self._maxsize,
            bar.timestamp,
        )
```
- **Bounded:** maxsize=2048 bars per symbol
- **Overflow:** Drops bars and logs warning
- **Backpressure:** Prevents memory growth if consumer stalls

**New (RedisClient):**
```python
def publish_bar(self, symbol: str, timeframe: str, bar_data: Dict[str, Any]) -> int:
    channel = f"market:{symbol}:{timeframe}"
    message = json.dumps(bar_data)
    num_subscribers = self.client.publish(channel, message)
    return num_subscribers
```
- **Unbounded:** No size limit
- **Overflow:** None (Redis pub/sub doesn't queue, just broadcasts)
- **Backpressure:** None (if subscriber can't keep up, messages are lost)

**‚ö†Ô∏è DIFFERENCE:** Redis pub/sub is "fire-and-forget". If a subscriber is slow or disconnected, it won't receive the message. This is fundamentally different from queues.

---

### 2. Consumer Queue Buffering

**Legacy (DatabentoLiveData):**
- Uses internal `RedisLiveData._queue` (Queue with maxsize=1024)
- Subscriber thread puts messages into queue
- `_load()` reads from queue with timeout

```python
# redis_feed.py:66
self._queue: queue.Queue = queue.Queue(maxsize=1024)

# redis_feed.py:207-213
try:
    self._queue.put_nowait(bar_data)
except queue.Full:
    logger.warning(
        "Queue full for %s, dropping bar",
        self.p.symbol
    )
```

**‚úÖ GOOD:** RedisLiveData does have a bounded queue (1024) with overflow handling!

---

### 3. Control Signals (RESET, SHUTDOWN)

**Legacy (QueueFanout):**
```python
@dataclasses.dataclass(frozen=True)
class QueueSignal:
    kind: str
    symbol: Optional[str] = None
    RESET = "reset"
    SHUTDOWN = "shutdown"

def publish_reset(self, symbol: str) -> None:
    q = self._ensure_queue(symbol)
    q.put_nowait(QueueSignal(QueueSignal.RESET, symbol))

def broadcast_shutdown(self) -> None:
    for symbol, q in self._queues.items():
        q.put_nowait(QueueSignal(QueueSignal.SHUTDOWN, symbol))
```

Consumer handles signals:
```python
# databento_bridge.py:776-785
if isinstance(payload, QueueSignal):
    if payload.kind == QueueSignal.SHUTDOWN:
        logger.info("Queue shutdown signalled for %s", self.p.symbol)
        self._stopped = True
        return False
    if payload.kind == QueueSignal.RESET:
        logger.info("Resetting feed state for %s", self.p.symbol)
        self._latest_dt = None
        return None
```

**New (Redis):**
- **‚ùå MISSING:** No equivalent to RESET signal
- **‚ùå MISSING:** No equivalent to SHUTDOWN signal
- Workers just stop when data hub disconnects

**Impact:**
- **RESET:** Legacy uses this to clear feed state when reconnecting. Not critical if we handle disconnects properly.
- **SHUTDOWN:** Legacy uses this for graceful shutdown. Workers should detect data hub going away and shut down gracefully.

---

### 4. Warmup Bar Handling

**Legacy (DatabentoLiveData):**
```python
# databento_bridge.py:754-765
if self._warmup_bars:
    self._qcheck = 0.0  # Fast drain
    payload = self._warmup_bars.popleft()
else:
    if not self._qcheck:
        self._qcheck = self.p.qcheck or 0.5  # Restore normal qcheck
```

**New (RedisLiveData):**
```python
# redis_feed.py:237-244
if self._warmup_bars:
    self._qcheck = 0.0  # Fast warmup
    bar_data = self._warmup_bars.popleft()
    return self._populate_lines(bar_data)

# Restore normal qcheck
if not self._qcheck:
    self._qcheck = self.p.qcheck or 0.5
```

**‚úÖ IDENTICAL:** Both drain warmup bars with qcheck=0, then restore normal qcheck.

---

### 5. Duplicate/Stale Bar Handling

**Legacy (DatabentoLiveData):**
```python
# databento_bridge.py:791-797
if self._latest_dt and payload.timestamp <= self._latest_dt:
    if not self.p.backfill:
        logger.debug("Skipping stale bar for %s @ %s", self.p.symbol, payload.timestamp)
        return None
```

**New (RedisLiveData):**
```python
# redis_feed.py:316-322
if self._latest_dt and timestamp <= self._latest_dt:
    logger.debug(
        "Skipping stale bar for %s @ %s",
        self.p.symbol,
        timestamp
    )
    return None
```

**‚úÖ IDENTICAL:** Both skip stale bars. Legacy has `backfill` parameter to allow stale bars (we removed this).

---

### 6. Contract Symbol Tracking

**Legacy (QueueFanout):**
```python
# databento_bridge.py:141-144
if bar.contract_symbol:
    with self._lock:
        self._root_to_contract_symbol[bar.symbol] = bar.contract_symbol

# databento_bridge.py:807-809 (consumer)
if payload.contract_symbol:
    self._current_contract_symbol = payload.contract_symbol
```
- QueueFanout tracks `_root_to_contract_symbol` mapping
- Bar includes `contract_symbol` field
- Consumer stores it in `_current_contract_symbol`

**New (DataHub):**
```python
# data_hub_main.py:168 (tracks in DataHub)
self._instrument_to_contract[instrument_id] = contract_symbol

# Bar data includes contract_symbol (but Redis doesn't use it)
# RedisLiveData doesn't track contract symbols at all
```

**‚ö†Ô∏è DIFFERENCE:** Contract symbol tracking exists in data hub but not exposed via Redis.

**Impact:** Probably not critical - contract symbols are mainly for logging/debugging.

---

### 7. Data Serialization

**Legacy:**
- Passes Python `Bar` objects directly (no serialization)
- Fast, no overhead

**New:**
- Serializes to JSON
- Small CPU overhead for serialize/deserialize
- Potential float precision issues (unlikely with default JSON encoder)

---

### 8. Timeout & Blocking Behavior

**Legacy (DatabentoLiveData):**
```python
# databento_bridge.py:767-774
timeout = self._qcheck or self.p.qcheck or 0.5
try:
    if timeout <= 0:
        payload = self._queue.get_nowait()
    else:
        payload = self._queue.get(timeout=timeout)
except queue.Empty:
    return None
```
- Blocking `queue.get(timeout=0.5)`
- Returns `None` if no data within timeout

**New (RedisLiveData):**
```python
# redis_feed.py:247-251
timeout = self._qcheck or 0.5
try:
    bar_data = self._queue.get(timeout=timeout)
except queue.Empty:
    return None  # No data yet
```

**‚úÖ IDENTICAL:** Both use `queue.get(timeout)` pattern.

---

### 9. Thread Safety

**Legacy:**
- QueueFanout uses `threading.Lock()` for queue creation and contract tracking
- `queue.Queue` is thread-safe by design

**New:**
- Redis handles concurrency internally
- Subscriber thread writes to local `queue.Queue` (thread-safe)
- No explicit locking needed

---

### 10. Reconnection & Error Handling

**Legacy:**
- DatabentoSubscriber handles reconnects
- Publishes QueueSignal.RESET on reconnect
- Consumers clear state on RESET

**New:**
- DataHub handles Databento reconnects
- Redis handles network failures transparently
- Workers auto-reconnect to Redis if connection drops
- **‚ùå MISSING:** No state reset signal to workers

---

## Risk Assessment

### ‚úÖ Low Risk Differences

1. **Unbounded pub/sub:** Redis pub/sub doesn't queue, but workers have bounded internal queues (1024)
2. **JSON serialization:** Minimal overhead, no precision issues observed
3. **Contract symbol tracking:** Not critical for strategy execution
4. **Thread safety:** Both approaches are thread-safe

### ‚ö†Ô∏è Medium Risk Differences

1. **No RESET signal:** Workers don't reset state when data hub reconnects
   - **Mitigation:** RedisLiveData already handles stale bars by checking timestamps
   - **Risk:** Low - timestamps prevent out-of-order issues

2. **No SHUTDOWN signal:** Workers don't know when data hub shuts down gracefully
   - **Mitigation:** Workers detect missing heartbeats and timeout
   - **Risk:** Low - workers will notice missing data and can shut down

3. **Pub/sub fire-and-forget:** If worker is disconnected, it misses bars
   - **Mitigation:** Workers reconnect quickly, warmup cache provides latest bar
   - **Risk:** Medium - could miss bars during network blip

### üÜï New Capabilities (Redis Advantages)

1. **Multi-timeframe pre-aggregation:** Data hub publishes 1min/hourly/daily
   - Legacy: Consumers aggregate minute bars themselves
   - New: Data hub does aggregation once, all workers benefit

2. **Horizontal scaling:** Workers can run on different machines
   - Legacy: Limited to single process
   - New: Can scale to multiple servers

3. **Monitoring:** Can inspect Redis channels to see what's being published
   - Legacy: Queues are opaque

---

## Recommendations

### ‚úÖ Keep Current Implementation

The Redis implementation is fundamentally sound and provides significant architectural advantages:
- Cleaner separation between data hub and workers
- Multi-alpha support with shared data stream
- Horizontal scaling capability

### üîß Optional Enhancements

1. **Add shutdown channel** for graceful worker shutdown:
   ```python
   # Data hub on shutdown:
   redis_client.publish("datahub:control", {"type": "shutdown"})

   # Workers subscribe to control channel and handle shutdown
   ```

2. **Add reconnect event** to trigger worker state reset:
   ```python
   # Data hub on reconnect:
   redis_client.publish("datahub:control", {"type": "reconnect", "timestamp": ...})

   # Workers could clear stale state if needed
   ```

3. **‚úÖ IMPLEMENTED: Monitor subscriber count** to detect disconnected workers:
   ```python
   num_subscribers = redis_client.publish_and_cache(symbol, timeframe, bar_data)
   if num_subscribers == 0:
       logger.warning("Published %s %s bar but NO SUBSCRIBERS are listening", symbol, timeframe)
   ```
   This is now implemented in `data_hub_main.py:329-334`

---

## Testing Checklist

- [x] Workers receive bars from Redis channels
- [x] Warmup bars drain with qcheck=0
- [x] Live bars use normal qcheck=0.5
- [x] Stale bars are skipped
- [x] Worker internal queue (1024) prevents overflow
- [ ] Workers handle Redis disconnect gracefully
- [ ] Workers detect data hub shutdown within reasonable time
- [ ] No bars are lost during normal operation
- [ ] Session boundaries (23:00 UTC) are correct
- [ ] Multiple workers can subscribe to same channels

---

## Summary

### What's the Same ‚úÖ

1. ‚úÖ Warmup bar handling (qcheck=0 drain, then restore)
2. ‚úÖ Stale bar detection (timestamp comparison)
3. ‚úÖ Consumer timeout pattern (queue.get with timeout)
4. ‚úÖ Bounded internal queue (1024 in Redis worker)
5. ‚úÖ Bar data structure (OHLCV + timestamp)

### What's Different üîÑ

1. üîÑ **Transport:** In-memory queues ‚Üí Redis pub/sub
2. üîÑ **Serialization:** Python objects ‚Üí JSON
3. üîÑ **Pre-aggregation:** Workers aggregate ‚Üí Data hub aggregates
4. üîÑ **Signals:** QueueSignal.RESET/SHUTDOWN ‚Üí None (could add control channel)
5. üîÑ **Scope:** Single-process ‚Üí Distributed

### What's Missing ‚ùå

1. ‚ùå RESET signal (low risk - timestamp check handles this)
2. ‚ùå SHUTDOWN signal (low risk - timeout detection works)
3. ‚ùå Backfill support (intentionally removed)

### Net Result üéØ

**The Redis implementation provides equivalent functionality to the Queue approach, with architectural improvements for multi-alpha support. The missing control signals have low risk, as the timestamp-based stale detection and timeout mechanisms provide equivalent safety.**
